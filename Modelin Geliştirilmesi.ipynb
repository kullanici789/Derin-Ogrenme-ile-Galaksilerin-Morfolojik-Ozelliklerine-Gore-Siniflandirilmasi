{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, mean_squared_error, confusion_matrix, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import (CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau)\n",
    "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, MaxPooling2D)\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import EfficientNetB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 32  \n",
    "SQL_DATABASE = \"data/galaxy_data.sqlite\"\n",
    "TRAIN_IMAGES_ARRAY_NPY = \"data/training_images_array.npy\"\n",
    "TEST_IMAGES_ARRAY_NPY = \"data/testing_images_array.npy\"\n",
    "\n",
    "TRAIN_IMAGES_DIR = \"data/train_images/\"\n",
    "TEST_IMAGES_DIR = \"data/test_images/\"\n",
    "MODEL_SAVE_PATH = \"data/model/GalaxyModel.keras\"\n",
    "TRAINING_LOG_PATH = \"data/model/training_log.csv\"\n",
    "CHECKPOINT_PATH = \"data/model/checkpoints/cp-{epoch:03d}.weights.h5\"\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(SQL_DATABASE)\n",
    "df_import = pd.read_sql(\"SELECT * from galaxy_data\", connection)\n",
    "connection.close()\n",
    "df_import.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import.dropna(inplace=True)\n",
    "stratify_data = df_import[\"class_reduced\"].values\n",
    "x_image_id_names = df_import[\"asset_id\"]\n",
    "y_output_data = df_import.drop([\"objid\", \"sample\", \"asset_id\", \"dr7objid\", \"ra\", \"dec\", \"gz2_class\", \"class_reduced\"], axis=1)\n",
    "y_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_assets, X_test_assets, y_train, y_test = train_test_split(x_image_id_names,\n",
    "                                                                  y_output_data,\n",
    "                                                                  random_state=RANDOM_STATE,\n",
    "                                                                  stratify=stratify_data)\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, image_list):\n",
    "    images = []\n",
    "    for img_name in image_list:\n",
    "        img_path = os.path.join(directory, f\"{img_name}.png\")\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        images.append(img.astype('float32') / 255.0)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(TRAIN_IMAGES_ARRAY_NPY).exists():\n",
    "    X_train_images = np.load(TRAIN_IMAGES_ARRAY_NPY, \"r\")\n",
    "else:\n",
    "    print(\"Creating `training_images_array.npy` file...\")\n",
    "    X_train_images = load_images(TRAIN_IMAGES_DIR, X_train_assets)\n",
    "    np.save(TRAIN_IMAGES_ARRAY_NPY, X_train_images)\n",
    "    \n",
    "if Path(TEST_IMAGES_ARRAY_NPY).exists():\n",
    "    X_test_images = np.load(TEST_IMAGES_ARRAY_NPY, \"r\")\n",
    "else:\n",
    "    print(\"Creating `testing_images_array.npy` file...\")\n",
    "    X_test_images = load_images(TEST_IMAGES_DIR, X_test_assets)\n",
    "    np.save(TEST_IMAGES_ARRAY_NPY, X_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_images Shape:\", X_train_images.shape)\n",
    "print(\"X_train_images Size\", X_train_images.nbytes, \"bytes\")\n",
    "print(\"y_train Shape:\", y_train.shape)\n",
    "print(\"y_test Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_images = X_train_images[..., np.newaxis]\n",
    "X_test_images = X_test_images[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = ModelCheckpoint(CHECKPOINT_PATH, monitor=\"val_loss\", save_weights_only=True, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6)\n",
    "csv_logger = CSVLogger(TRAINING_LOG_PATH, append=True)\n",
    "\n",
    "callbacks = [checkpoints, early_stopping, reduce_lr_plateau, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_true, y_pred):\n",
    "    SS_res = tf.reduce_sum(tf.square(y_true - y_pred)) \n",
    "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred) \n",
    "    tp = tf.reduce_sum(y_true * y_pred, axis=0)\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "    f1 = tf.reduce_mean(f1)\n",
    "    return f1\n",
    "\n",
    "base_model = ResNet50(include_top=False,weights=None, input_shape=(128, 128, 1))\n",
    "base_model.trainable = True\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.3), \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3), \n",
    "    Dense(37, activation='sigmoid') \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy', f1_score] \n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_images, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_images, y_train,\n",
    "    validation_split=0.1,\n",
    "    initial_epoch=50,\n",
    "    epochs=120,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred, threshold=0.5):\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred_binary, average='weighted')\n",
    "    bce = log_loss(y_true, y_pred, eps=1e-7)\n",
    "    return f1, bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if 'binary_accuracy' in history.history:\n",
    "        plt.plot(history.history['binary_accuracy'], label='Train Binary Accuracy')\n",
    "        plt.plot(history.history['val_binary_accuracy'], label='Validation Binary Accuracy')\n",
    "    else:\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Train vs Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Train vs Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, bce = evaluate_metrics(y_test, y_pred)\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "print(\"\\nTest Sonuçları:\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Binary Cross-Entropy (BCE): {bce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image_path: str, square_size: int=106) -> np.array:\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if(image.shape[0] > image.shape[1]):\n",
    "        r = square_size / float(image.shape[0])\n",
    "        dim = (int(image.shape[1] * r), square_size)\n",
    "    else:\n",
    "        r = square_size / float(image.shape[1])\n",
    "        dim = (square_size, int(image.shape[0] * r))\n",
    "\n",
    "    resized = cv2.resize(image, dim, interpolation=cv2.INTER_CUBIC)\n",
    "    resized = np.expand_dims(resized, axis=2)  \n",
    "    prepared_image = np.full((square_size, square_size, 1), 7, np.uint8) \n",
    "    x_offset = (square_size - resized.shape[1]) // 2\n",
    "    y_offset = (square_size - resized.shape[0]) // 2\n",
    "    prepared_image[y_offset:y_offset+resized.shape[0], x_offset:x_offset+resized.shape[1]] = resized\n",
    "    predict_img = prepared_image.astype(\"float32\") / 255.0\n",
    "\n",
    "    return predict_img\n",
    "\n",
    "def predict_image(file_path: str) -> pd.Series:\n",
    "    image_array = prepare_image(file_path)\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "    predictions = model.predict(image_array)\n",
    "    readable_predict = pd.Series(predictions[0], y_output_data.columns.values)\n",
    "    return readable_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r\"images\\Ex_SBb_NGC1300-HST.jpg\"\n",
    "plt.imshow(plt.imread(test_image_path))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "result = predict_image(test_image_path)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
